{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TESIS - MODELADO (Baseline + Tuning + Tuned)\n",
    "# Dataset: Give Me Some Credit (cs-training.csv)\n",
    "# Evaluación: métricas en TEST, 10 seeds (robustez)\n",
    "# Tuning: complementario, 1 vez (seed_ref), CV interna con SMOTE en pipeline\n",
    "# Outputs: resultados largos, resumen (12 filas), grupo, deltas, curvas seed_ref\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "from scipy.special import expit  # sigmoid, para convertir decision_function a pseudo-proba\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIGURACIÓN GENERAL\n",
    "# ============================================================\n",
    "\n",
    "# --- Usa todos los núcleos cuando sea posible ---\n",
    "# (Algunas libs respetan OMP_NUM_THREADS; otras usan n_jobs=-1)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count() or 8)\n",
    "\n",
    "DATA_PATH = \"GiveMeSomeCredit/cs-training.csv\"\n",
    "SEP = \";\"\n",
    "INDEX_COL = 0\n",
    "\n",
    "SEEDS = list(range(42, 52))      # 10 iteraciones: 42..51\n",
    "SEED_REF = 42                    # seed de referencia para tuning + curvas\n",
    "\n",
    "THRESHOLD = 0.5                  # umbral fijo (tesis: comparabilidad)\n",
    "POS_LABEL = 1                    # clase positiva = incumplimiento (1)\n",
    "\n",
    "# Banderas: corre por partes sin rehacer todo\n",
    "RUN_BASELINE = True\n",
    "RUN_TUNING   = True\n",
    "RUN_TUNED    = True\n",
    "\n",
    "# Carpeta de salidas\n",
    "OUT_DIR = \"salidas_tesis\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Archivos de salida\n",
    "BASELINE_LONG_CSV = os.path.join(OUT_DIR, \"results_baseline_long.csv\")\n",
    "TUNED_LONG_CSV    = os.path.join(OUT_DIR, \"results_tuned_long.csv\")\n",
    "SUMMARY_BASE_CSV  = os.path.join(OUT_DIR, \"results_baseline_summary_12rows.csv\")\n",
    "SUMMARY_TUNED_CSV = os.path.join(OUT_DIR, \"results_tuned_summary_12rows.csv\")\n",
    "GROUP_BASE_CSV    = os.path.join(OUT_DIR, \"results_baseline_group_summary.csv\")\n",
    "GROUP_TUNED_CSV   = os.path.join(OUT_DIR, \"results_tuned_group_summary.csv\")\n",
    "DELTAS_BASE_CSV   = os.path.join(OUT_DIR, \"deltas_baseline_ens_minus_no.csv\")\n",
    "DELTAS_TUNED_CSV  = os.path.join(OUT_DIR, \"deltas_tuned_ens_minus_no.csv\")\n",
    "\n",
    "TUNING_REPORT_CSV = os.path.join(OUT_DIR, \"tuning_report.csv\")\n",
    "BEST_PARAMS_JSON  = os.path.join(OUT_DIR, \"best_params.json\")\n",
    "\n",
    "# Curvas para gráficos (solo seed_ref y 2 modelos: mejor ensamble + mejor no-ensamble)\n",
    "CURVES_BASELINE_CSV = os.path.join(OUT_DIR, \"curves_seed42_baseline.csv\")\n",
    "CURVES_TUNED_CSV    = os.path.join(OUT_DIR, \"curves_seed42_tuned.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) PIPELINE DE DATOS (Fase 3) - por seed\n",
    "# ============================================================\n",
    "\n",
    "def load_and_split(seed: int) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Lee dataset, separa X/y y hace split 70/30 estratificado.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(DATA_PATH, sep=SEP, index_col=INDEX_COL)\n",
    "    y = df[\"SeriousDlqin2yrs\"]\n",
    "    X = df.drop(columns=[\"SeriousDlqin2yrs\"])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.30, random_state=seed, stratify=y\n",
    "    )\n",
    "    return X_train.copy(), y_train.copy(), X_test.copy(), y_test.copy()\n",
    "\n",
    "\n",
    "def preprocess_fit_transform(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    seed: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Preprocesa:\n",
    "    - age=0 -> NA\n",
    "    - imputación por mediana (solo train)\n",
    "    - winsorización p1–p99 (límites solo train)\n",
    "    - RobustScaler (fit train, transform test)\n",
    "    - SMOTE solo en train\n",
    "    Devuelve:\n",
    "    - X_train_res, y_train_res, X_test_scaled\n",
    "    - metadata de test (n_test, pos_rate_test)\n",
    "    \"\"\"\n",
    "    # 1) age=0 -> NA\n",
    "    X_train.loc[X_train[\"age\"] == 0, \"age\"] = pd.NA\n",
    "    X_test.loc[X_test[\"age\"] == 0, \"age\"] = pd.NA\n",
    "\n",
    "    # 2) imputación por mediana (aprendida en train)\n",
    "    med_income = X_train[\"MonthlyIncome\"].median()\n",
    "    med_deps   = X_train[\"NumberOfDependents\"].median()\n",
    "    med_age    = X_train[\"age\"].median()\n",
    "\n",
    "    X_train[\"MonthlyIncome\"] = X_train[\"MonthlyIncome\"].fillna(med_income)\n",
    "    X_test[\"MonthlyIncome\"]  = X_test[\"MonthlyIncome\"].fillna(med_income)\n",
    "\n",
    "    X_train[\"NumberOfDependents\"] = X_train[\"NumberOfDependents\"].fillna(med_deps)\n",
    "    X_test[\"NumberOfDependents\"]  = X_test[\"NumberOfDependents\"].fillna(med_deps)\n",
    "\n",
    "    X_train[\"age\"] = X_train[\"age\"].fillna(med_age)\n",
    "    X_test[\"age\"]  = X_test[\"age\"].fillna(med_age)\n",
    "\n",
    "    # 3) winsorización p1–p99 (límites del train)\n",
    "    vars_out = [\"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\", \"MonthlyIncome\"]\n",
    "    for col in vars_out:\n",
    "        lo = X_train[col].quantile(0.01)\n",
    "        hi = X_train[col].quantile(0.99)\n",
    "        X_train[col] = X_train[col].clip(lo, hi)\n",
    "        X_test[col]  = X_test[col].clip(lo, hi)\n",
    "\n",
    "    # 4) escalado robusto (fit train, transform test)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # 5) SMOTE solo en train\n",
    "    smote = SMOTE(random_state=seed)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    meta = {\n",
    "        \"n_test\": int(len(y_train) * 0 + len(X_test)),  # simple\n",
    "        \"pos_rate_test\": float(y_train*0 + y_train*0 + y_train*0 + y_train*0 + 0)  # placeholder\n",
    "    }\n",
    "    # Corrige meta de forma explícita:\n",
    "    meta[\"n_test\"] = int(len(X_test))\n",
    "    meta[\"pos_rate_test\"] = float((y_train*0 + 0).sum())  # dummy reset\n",
    "    meta[\"pos_rate_test\"] = float(y_train*0 + 0)          # dummy reset\n",
    "    meta[\"pos_rate_test\"] = float((y_train*0 + 0).sum())  # dummy reset\n",
    "    meta[\"pos_rate_test\"] = float((y_train*0 + 0).sum())  # dummy reset\n",
    "    # realmente:\n",
    "    meta[\"pos_rate_test\"] = float((y_test_global := None) or 0.0)  # placeholder\n",
    "\n",
    "    # Para no depender de variable externa, retorna sin pos_rate_test aquí;\n",
    "    # lo calcularemos afuera con y_test real (más limpio).\n",
    "    return X_train_res, np.asarray(y_train_res), X_test_scaled, {}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) MÉTRICAS EN TEST (Fase 5)\n",
    "# ============================================================\n",
    "\n",
    "def get_test_scores(model, X_test: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Devuelve score continuo para ROC-AUC y AP:\n",
    "    - si predict_proba: probas clase 1\n",
    "    - si decision_function: raw scores\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X_test)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(X_test)\n",
    "    raise ValueError(\"Modelo sin predict_proba ni decision_function.\")\n",
    "\n",
    "\n",
    "def scores_to_labels(scores: np.ndarray, threshold: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convierte scores a labels con umbral fijo.\n",
    "    - Si scores ya son probas: ok\n",
    "    - Si scores son raw (SVM): aplicamos sigmoid como aproximación\n",
    "    \"\"\"\n",
    "    # Heurística: si hay valores fuera de [0,1], asumimos raw scores\n",
    "    if np.nanmin(scores) < 0 or np.nanmax(scores) > 1:\n",
    "        probs = expit(scores)  # convierte a (0,1)\n",
    "        return (probs >= threshold).astype(int)\n",
    "    return (scores >= threshold).astype(int)\n",
    "\n",
    "\n",
    "def evaluate_on_test(\n",
    "    model,\n",
    "    X_train: np.ndarray, y_train: np.ndarray,\n",
    "    X_test: np.ndarray, y_test: np.ndarray,\n",
    "    threshold: float\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Entrena en train y evalúa en test con métricas definidas.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_seconds = time.time() - t0\n",
    "\n",
    "    scores = get_test_scores(model, X_test)\n",
    "    y_pred = scores_to_labels(scores, threshold=threshold)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    # Métricas (todas en TEST)\n",
    "    roc_auc = float(roc_auc_score(y_test, scores))\n",
    "    ap      = float(average_precision_score(y_test, scores))\n",
    "\n",
    "    bal_acc = float(balanced_accuracy_score(y_test, y_pred))\n",
    "    prec    = float(precision_score(y_test, y_pred, zero_division=0))\n",
    "    rec     = float(recall_score(y_test, y_pred, zero_division=0))\n",
    "    f1      = float(f1_score(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return {\n",
    "        \"tp\": int(tp), \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn),\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"average_precision\": ap,\n",
    "        \"fit_seconds\": float(fit_seconds),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) DEFINICIÓN DE MODELOS\n",
    "# ============================================================\n",
    "\n",
    "def baseline_models(seed: int) -> List[Tuple[str, str, Any]]:\n",
    "    \"\"\"\n",
    "    Baseline = configuración razonable (no “default puro”), estable y defendible.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (\"Logistic Regression\", \"No Ensemble\",\n",
    "         LogisticRegression(max_iter=2000, random_state=seed, n_jobs=-1)),\n",
    "\n",
    "        (\"Decision Tree\", \"No Ensemble\",\n",
    "         DecisionTreeClassifier(random_state=seed)),\n",
    "\n",
    "        (\"SVM (Lineal)\", \"No Ensemble\",\n",
    "         LinearSVC(random_state=seed)),\n",
    "\n",
    "        (\"Random Forest\", \"Ensemble\",\n",
    "         RandomForestClassifier(\n",
    "             n_estimators=300,\n",
    "             random_state=seed,\n",
    "             n_jobs=-1\n",
    "         )),\n",
    "\n",
    "        (\"Bagging (Decision Tree)\", \"Ensemble\",\n",
    "         BaggingClassifier(\n",
    "             estimator=DecisionTreeClassifier(random_state=seed),\n",
    "             n_estimators=300,\n",
    "             random_state=seed,\n",
    "             n_jobs=-1\n",
    "         )),\n",
    "\n",
    "        (\"XGBoost\", \"Ensemble\",\n",
    "         XGBClassifier(\n",
    "             n_estimators=400,\n",
    "             learning_rate=0.05,\n",
    "             max_depth=4,\n",
    "             subsample=0.8,\n",
    "             colsample_bytree=0.8,\n",
    "             reg_lambda=1.0,\n",
    "             random_state=seed,\n",
    "             n_jobs=-1,\n",
    "             eval_metric=\"logloss\"\n",
    "         )),\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_model_from_params(model_key: str, seed: int, params: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Crea modelo a partir de best_params guardados.\n",
    "    \"\"\"\n",
    "    if model_key == \"Logistic Regression\":\n",
    "        return LogisticRegression(max_iter=2000, random_state=seed, n_jobs=-1, **params)\n",
    "\n",
    "    if model_key == \"Decision Tree\":\n",
    "        return DecisionTreeClassifier(random_state=seed, **params)\n",
    "\n",
    "    if model_key == \"SVM (Lineal)\":\n",
    "        return LinearSVC(random_state=seed, **params)\n",
    "\n",
    "    if model_key == \"Random Forest\":\n",
    "        return RandomForestClassifier(random_state=seed, n_jobs=-1, **params)\n",
    "\n",
    "    if model_key == \"Bagging (Decision Tree)\":\n",
    "        base_params = params.pop(\"base_estimator_params\", {})\n",
    "        estimator = DecisionTreeClassifier(random_state=seed, **base_params)\n",
    "        return BaggingClassifier(estimator=estimator, random_state=seed, n_jobs=-1, **params)\n",
    "\n",
    "    if model_key == \"XGBoost\":\n",
    "        return XGBClassifier(random_state=seed, n_jobs=-1, eval_metric=\"logloss\", **params)\n",
    "\n",
    "    raise ValueError(f\"Modelo desconocido: {model_key}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) TUNING COMPLEMENTARIO (1 vez, seed_ref)\n",
    "#    - CV interna\n",
    "#    - SMOTE dentro del pipeline para evitar leakage\n",
    "#    - scoring = average_precision\n",
    "# ============================================================\n",
    "\n",
    "def tuning_search_spaces() -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Espacios de búsqueda moderados (controlados) para no demorar eternidad.\n",
    "    Nota: Bagging necesita params del estimador base aparte.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"Logistic Regression\": {\n",
    "            \"model__C\": np.logspace(-4, 2, 20),\n",
    "            \"model__penalty\": [\"l2\"],\n",
    "            \"model__solver\": [\"lbfgs\"],\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            \"model__max_depth\": [None, 3, 4, 5, 6, 8, 10],\n",
    "            \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4, 8],\n",
    "        },\n",
    "        \"SVM (Lineal)\": {\n",
    "            \"model__C\": np.logspace(-4, 2, 20),\n",
    "        },\n",
    "        \"Random Forest\": {\n",
    "            \"model__n_estimators\": [300, 500, 800],\n",
    "            \"model__max_features\": [\"sqrt\", \"log2\", None],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],\n",
    "            \"model__max_depth\": [None, 5, 8, 12],\n",
    "        },\n",
    "        \"Bagging (Decision Tree)\": {\n",
    "            \"model__n_estimators\": [300, 500, 800],\n",
    "            \"model__max_samples\": [0.6, 0.8, 1.0],\n",
    "            \"model__max_features\": [0.6, 0.8, 1.0],\n",
    "            \"model__estimator__max_depth\": [None, 3, 5, 8],\n",
    "            \"model__estimator__min_samples_leaf\": [1, 2, 4, 8],\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model__n_estimators\": [300, 500, 700],\n",
    "            \"model__max_depth\": [3, 4, 5],\n",
    "            \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "            \"model__subsample\": [0.7, 0.8, 0.9],\n",
    "            \"model__colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "            \"model__min_child_weight\": [1, 3, 5],\n",
    "            \"model__reg_lambda\": [1.0, 2.0, 5.0],\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def tune_models_once(seed_ref: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tuning complementario usando el split del seed_ref.\n",
    "    Importante: tuning se hace SOLO con train (CV interna), nunca con test.\n",
    "    Retorna best_params por modelo.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== TUNING COMPLEMENTARIO (solo 1 vez) ===\")\n",
    "    X_tr_raw, y_tr, X_te_raw, y_te = load_and_split(seed_ref)\n",
    "\n",
    "    # Preprocesamiento “sin SMOTE” aquí, porque SMOTE irá dentro del pipeline de CV.\n",
    "    # Pero sí hacemos imputación/winsor/escalado con fit en train, transform test.\n",
    "    # Para tuning solo usaremos el TRAIN transformado.\n",
    "    # Implementamos un mini-preprocess reutilizando la misma lógica (sin SMOTE final).\n",
    "    # --- age=0 -> NA ---\n",
    "    X_tr_raw.loc[X_tr_raw[\"age\"] == 0, \"age\"] = pd.NA\n",
    "    X_te_raw.loc[X_te_raw[\"age\"] == 0, \"age\"] = pd.NA\n",
    "\n",
    "    # --- imputación por mediana (train) ---\n",
    "    med_income = X_tr_raw[\"MonthlyIncome\"].median()\n",
    "    med_deps   = X_tr_raw[\"NumberOfDependents\"].median()\n",
    "    med_age    = X_tr_raw[\"age\"].median()\n",
    "\n",
    "    for X_ in (X_tr_raw, X_te_raw):\n",
    "        X_[\"MonthlyIncome\"] = X_[\"MonthlyIncome\"].fillna(med_income)\n",
    "        X_[\"NumberOfDependents\"] = X_[\"NumberOfDependents\"].fillna(med_deps)\n",
    "        X_[\"age\"] = X_[\"age\"].fillna(med_age)\n",
    "\n",
    "    # --- winsor p1-p99 (train) ---\n",
    "    vars_out = [\"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\", \"MonthlyIncome\"]\n",
    "    for col in vars_out:\n",
    "        lo = X_tr_raw[col].quantile(0.01)\n",
    "        hi = X_tr_raw[col].quantile(0.99)\n",
    "        X_tr_raw[col] = X_tr_raw[col].clip(lo, hi)\n",
    "        X_te_raw[col] = X_te_raw[col].clip(lo, hi)\n",
    "\n",
    "    # --- escalado robusto ---\n",
    "    scaler = RobustScaler()\n",
    "    X_tr = scaler.fit_transform(X_tr_raw)\n",
    "    # X_te no se usa en tuning; se deja para coherencia\n",
    "    _ = scaler.transform(X_te_raw)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed_ref)\n",
    "    spaces = tuning_search_spaces()\n",
    "\n",
    "    base_defs = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=2000, random_state=seed_ref, n_jobs=-1),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=seed_ref),\n",
    "        \"SVM (Lineal)\": LinearSVC(random_state=seed_ref),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=seed_ref, n_jobs=-1),\n",
    "        \"Bagging (Decision Tree)\": BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(random_state=seed_ref),\n",
    "            random_state=seed_ref,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \"XGBoost\": XGBClassifier(random_state=seed_ref, n_jobs=-1, eval_metric=\"logloss\"),\n",
    "    }\n",
    "\n",
    "    best_params = {}\n",
    "    report_rows = []\n",
    "\n",
    "    for model_name, base_model in base_defs.items():\n",
    "        print(f\"\\n[TUNING] {model_name}\")\n",
    "\n",
    "        # Pipeline con SMOTE dentro de CV (evita leakage)\n",
    "        pipe = ImbPipeline(steps=[\n",
    "            (\"smote\", SMOTE(random_state=seed_ref)),\n",
    "            (\"model\", base_model)\n",
    "        ])\n",
    "\n",
    "        param_dist = spaces[model_name]\n",
    "\n",
    "        # n_iter moderado; puedes subir si quieres más búsqueda\n",
    "        n_iter = 25 if model_name in [\"XGBoost\", \"Random Forest\", \"Bagging (Decision Tree)\"] else 20\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=n_iter,\n",
    "            scoring=\"average_precision\",\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            random_state=seed_ref,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        t0 = time.time()\n",
    "        search.fit(X_tr, y_tr)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        best_score = float(search.best_score_)\n",
    "        best = search.best_params_\n",
    "\n",
    "        # Limpieza de keys: model__X o model__estimator__X\n",
    "        cleaned = {}\n",
    "        base_estimator_params = {}\n",
    "\n",
    "        for k, v in best.items():\n",
    "            if k.startswith(\"model__estimator__\"):\n",
    "                base_estimator_params[k.replace(\"model__estimator__\", \"\")] = v\n",
    "            elif k.startswith(\"model__\"):\n",
    "                cleaned[k.replace(\"model__\", \"\")] = v\n",
    "            else:\n",
    "                # smote params u otros (normalmente no buscamos)\n",
    "                pass\n",
    "\n",
    "        if model_name == \"Bagging (Decision Tree)\":\n",
    "            cleaned[\"base_estimator_params\"] = base_estimator_params\n",
    "\n",
    "        best_params[model_name] = cleaned\n",
    "\n",
    "        report_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"best_cv_average_precision\": best_score,\n",
    "            \"n_iter\": n_iter,\n",
    "            \"cv_folds\": 5,\n",
    "            \"elapsed_seconds\": float(elapsed),\n",
    "            \"best_params\": json.dumps(cleaned)\n",
    "        })\n",
    "\n",
    "        print(\"  best CV AP:\", round(best_score, 6))\n",
    "        print(\"  best params:\", cleaned)\n",
    "\n",
    "    # Guardar outputs de tuning\n",
    "    pd.DataFrame(report_rows).to_csv(TUNING_REPORT_CSV, index=False)\n",
    "    with open(BEST_PARAMS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    print(\"\\nGuardado:\", TUNING_REPORT_CSV)\n",
    "    print(\"Guardado:\", BEST_PARAMS_JSON)\n",
    "    return best_params\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) EJECUCIÓN: BASELINE / TUNED (10 seeds)\n",
    "# ============================================================\n",
    "\n",
    "def run_experiment(config_name: str, best_params: Dict[str, Any] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Corre 10 seeds y 6 modelos.\n",
    "    config_name: \"Baseline\" o \"Tuned\"\n",
    "    best_params: dict de parámetros tuned (si config_name == \"Tuned\")\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    print(f\"\\n=== RUN: {config_name} ===\")\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        print(f\"\\n[SEED {seed}] preparando datos...\")\n",
    "        X_train_raw, y_train, X_test_raw, y_test = load_and_split(seed)\n",
    "\n",
    "        # Preprocesamiento completo (incluye SMOTE en train)\n",
    "        # Nota: pos_rate_test se calcula con y_test real acá\n",
    "        X_train_res, y_train_res, X_test_scaled, _ = preprocess_fit_transform(\n",
    "            X_train_raw, y_train, X_test_raw, seed\n",
    "        )\n",
    "        n_test = int(len(y_test))\n",
    "        pos_rate_test = float(np.mean(y_test))\n",
    "\n",
    "        # Modelos\n",
    "        if config_name == \"Baseline\":\n",
    "            models = baseline_models(seed)\n",
    "        else:\n",
    "            assert best_params is not None, \"Faltan best_params para Tuned.\"\n",
    "            models = []\n",
    "            for model_key in best_params.keys():\n",
    "                etiqueta = \"Ensemble\" if model_key in [\"Random Forest\", \"Bagging (Decision Tree)\", \"XGBoost\"] else \"No Ensemble\"\n",
    "                m = build_model_from_params(model_key, seed, dict(best_params[model_key]))  # copia\n",
    "                models.append((model_key, etiqueta, m))\n",
    "\n",
    "        # Evaluar\n",
    "        for model_name, group, model in models:\n",
    "            met = evaluate_on_test(\n",
    "                model=model,\n",
    "                X_train=X_train_res, y_train=y_train_res,\n",
    "                X_test=X_test_scaled, y_test=np.asarray(y_test),\n",
    "                threshold=THRESHOLD\n",
    "            )\n",
    "            row = {\n",
    "                \"seed\": seed,\n",
    "                \"config\": config_name,\n",
    "                \"model\": model_name,\n",
    "                \"group\": group,\n",
    "                \"threshold\": THRESHOLD,\n",
    "                \"pos_label\": POS_LABEL,\n",
    "                \"n_test\": n_test,\n",
    "                \"pos_rate_test\": pos_rate_test,\n",
    "                **met\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6) RESÚMENES PARA \"FICHA DETALLE\" Y \"FICHA RESUMEN\"\n",
    "# ============================================================\n",
    "\n",
    "METRIC_COLS = [\n",
    "    \"balanced_accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\", \"average_precision\"\n",
    "]\n",
    "\n",
    "def make_summary_12rows(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    12 filas = 6 modelos × 2 configs si se mezcla, pero aquí se espera un df de una config.\n",
    "    Entonces: 6 filas por config. (Si quieres 12, concatena baseline+tuned)\n",
    "    Reporta mean±std y median[IQR] por métrica.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for model in sorted(df_long[\"model\"].unique()):\n",
    "        sub = df_long[df_long[\"model\"] == model]\n",
    "        row = {\n",
    "            \"config\": sub[\"config\"].iloc[0],\n",
    "            \"model\": model,\n",
    "            \"group\": sub[\"group\"].iloc[0],\n",
    "            \"n_seeds\": int(sub[\"seed\"].nunique())\n",
    "        }\n",
    "        for m in METRIC_COLS:\n",
    "            vals = sub[m].values\n",
    "            row[f\"{m}_mean\"] = float(np.mean(vals))\n",
    "            row[f\"{m}_std\"]  = float(np.std(vals, ddof=1))\n",
    "            row[f\"{m}_median\"] = float(np.median(vals))\n",
    "            q1 = float(np.quantile(vals, 0.25))\n",
    "            q3 = float(np.quantile(vals, 0.75))\n",
    "            row[f\"{m}_q1\"] = q1\n",
    "            row[f\"{m}_q3\"] = q3\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def make_group_summary(df_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resumen por grupo (Ensemble vs No Ensemble), por métrica.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for grp in [\"Ensemble\", \"No Ensemble\"]:\n",
    "        sub = df_long[df_long[\"group\"] == grp]\n",
    "        for m in METRIC_COLS:\n",
    "            rows.append({\n",
    "                \"config\": sub[\"config\"].iloc[0],\n",
    "                \"group\": grp,\n",
    "                \"metric\": m,\n",
    "                \"mean\": float(sub[m].mean()),\n",
    "                \"std\": float(sub[m].std(ddof=1)),\n",
    "                \"median\": float(sub[m].median()),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def make_deltas_ens_minus_no(df_long: pd.DataFrame, metrics_for_inferential: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Genera tabla de deltas por seed:\n",
    "    delta(seed, metric) = mean(ensamble metric) - mean(no ensamble metric)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cfg = df_long[\"config\"].iloc[0]\n",
    "    for seed in sorted(df_long[\"seed\"].unique()):\n",
    "        dseed = df_long[df_long[\"seed\"] == seed]\n",
    "        ens = dseed[dseed[\"group\"] == \"Ensemble\"]\n",
    "        no  = dseed[dseed[\"group\"] == \"No Ensemble\"]\n",
    "        for m in metrics_for_inferential:\n",
    "            delta = float(ens[m].mean() - no[m].mean())\n",
    "            rows.append({\"config\": cfg, \"seed\": seed, \"metric\": m, \"delta_ens_minus_no\": delta})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) CURVAS PR/ROC (seed_ref + 2 modelos)\n",
    "# ============================================================\n",
    "\n",
    "def get_seed_ref_curves(df_long: pd.DataFrame, config_name: str, best_params: Dict[str, Any] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Para seed_ref:\n",
    "    - identifica mejor ensamble y mejor no-ensamble por AP (average_precision)\n",
    "    - recalcula scores en test para esos 2 modelos\n",
    "    - guarda y_test y score en formato largo: (y_true, score, model, group)\n",
    "    \"\"\"\n",
    "    df_ref = df_long[df_long[\"seed\"] == SEED_REF].copy()\n",
    "\n",
    "    # Mejor por AP dentro de cada grupo\n",
    "    best_ens = df_ref[df_ref[\"group\"] == \"Ensemble\"].sort_values(\"average_precision\", ascending=False).iloc[0][\"model\"]\n",
    "    best_no  = df_ref[df_ref[\"group\"] == \"No Ensemble\"].sort_values(\"average_precision\", ascending=False).iloc[0][\"model\"]\n",
    "\n",
    "    chosen = [(best_no, \"No Ensemble\"), (best_ens, \"Ensemble\")]\n",
    "    print(f\"\\n[CURVAS {config_name}] seed={SEED_REF} | best_no={best_no} | best_ens={best_ens}\")\n",
    "\n",
    "    # Prepara datos del seed_ref\n",
    "    X_train_raw, y_train, X_test_raw, y_test = load_and_split(SEED_REF)\n",
    "    X_train_res, y_train_res, X_test_scaled, _ = preprocess_fit_transform(\n",
    "        X_train_raw, y_train, X_test_raw, SEED_REF\n",
    "    )\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    curves_rows = []\n",
    "    for model_name, group in chosen:\n",
    "        if config_name == \"Baseline\":\n",
    "            # construye baseline específico\n",
    "            model_map = {name: m for (name, _, m) in baseline_models(SEED_REF)}\n",
    "            model = model_map[model_name]\n",
    "        else:\n",
    "            assert best_params is not None\n",
    "            model = build_model_from_params(model_name, SEED_REF, dict(best_params[model_name]))\n",
    "\n",
    "        model.fit(X_train_res, y_train_res)\n",
    "        scores = get_test_scores(model, X_test_scaled)\n",
    "\n",
    "        # guardamos todo el vector para curvas\n",
    "        for yt, sc in zip(y_test, scores):\n",
    "            curves_rows.append({\n",
    "                \"config\": config_name,\n",
    "                \"seed\": SEED_REF,\n",
    "                \"model\": model_name,\n",
    "                \"group\": group,\n",
    "                \"y_true\": int(yt),\n",
    "                \"y_score\": float(sc)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(curves_rows)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8) MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    best_params = None\n",
    "\n",
    "    # --- BASELINE ---\n",
    "    if RUN_BASELINE:\n",
    "        df_base = run_experiment(config_name=\"Baseline\")\n",
    "        df_base.to_csv(BASELINE_LONG_CSV, index=False)\n",
    "        print(\"\\nGuardado:\", BASELINE_LONG_CSV)\n",
    "\n",
    "        # Resumen 6 filas (baseline) + grupo + deltas\n",
    "        sum_base = make_summary_12rows(df_base)\n",
    "        sum_base.to_csv(SUMMARY_BASE_CSV, index=False)\n",
    "\n",
    "        grp_base = make_group_summary(df_base)\n",
    "        grp_base.to_csv(GROUP_BASE_CSV, index=False)\n",
    "\n",
    "        deltas_base = make_deltas_ens_minus_no(\n",
    "            df_base,\n",
    "            metrics_for_inferential=[\"average_precision\", \"roc_auc\", \"f1\", \"recall\"]\n",
    "        )\n",
    "        deltas_base.to_csv(DELTAS_BASE_CSV, index=False)\n",
    "\n",
    "        print(\"Guardado:\", SUMMARY_BASE_CSV)\n",
    "        print(\"Guardado:\", GROUP_BASE_CSV)\n",
    "        print(\"Guardado:\", DELTAS_BASE_CSV)\n",
    "    else:\n",
    "        df_base = pd.read_csv(BASELINE_LONG_CSV)\n",
    "\n",
    "    # --- TUNING COMPLEMENTARIO ---\n",
    "    if RUN_TUNING:\n",
    "        best_params = tune_models_once(seed_ref=SEED_REF)\n",
    "    else:\n",
    "        with open(BEST_PARAMS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            best_params = json.load(f)\n",
    "\n",
    "    # --- TUNED ---\n",
    "    if RUN_TUNED:\n",
    "        df_tuned = run_experiment(config_name=\"Tuned\", best_params=best_params)\n",
    "        df_tuned.to_csv(TUNED_LONG_CSV, index=False)\n",
    "        print(\"\\nGuardado:\", TUNED_LONG_CSV)\n",
    "\n",
    "        sum_tuned = make_summary_12rows(df_tuned)\n",
    "        sum_tuned.to_csv(SUMMARY_TUNED_CSV, index=False)\n",
    "\n",
    "        grp_tuned = make_group_summary(df_tuned)\n",
    "        grp_tuned.to_csv(GROUP_TUNED_CSV, index=False)\n",
    "\n",
    "        deltas_tuned = make_deltas_ens_minus_no(\n",
    "            df_tuned,\n",
    "            metrics_for_inferential=[\"average_precision\", \"roc_auc\", \"f1\", \"recall\"]\n",
    "        )\n",
    "        deltas_tuned.to_csv(DELTAS_TUNED_CSV, index=False)\n",
    "\n",
    "        print(\"Guardado:\", SUMMARY_TUNED_CSV)\n",
    "        print(\"Guardado:\", GROUP_TUNED_CSV)\n",
    "        print(\"Guardado:\", DELTAS_TUNED_CSV)\n",
    "    else:\n",
    "        df_tuned = pd.read_csv(TUNED_LONG_CSV)\n",
    "\n",
    "    # --- CURVAS (seed_ref) para gráficos PR/ROC ---\n",
    "    # Se hacen al final porque elegimos \"mejor modelo\" según resultados.\n",
    "    curves_base = get_seed_ref_curves(df_base, config_name=\"Baseline\", best_params=None)\n",
    "    curves_base.to_csv(CURVES_BASELINE_CSV, index=False)\n",
    "    print(\"Guardado:\", CURVES_BASELINE_CSV)\n",
    "\n",
    "    curves_tuned = get_seed_ref_curves(df_tuned, config_name=\"Tuned\", best_params=best_params)\n",
    "    curves_tuned.to_csv(CURVES_TUNED_CSV, index=False)\n",
    "    print(\"Guardado:\", CURVES_TUNED_CSV)\n",
    "\n",
    "    # --- FICHA DETALLE y FICHA RESUMEN ---\n",
    "    # Ficha detalle = df_long (baseline y tuned por separado o concatenado)\n",
    "    df_all = pd.concat([df_base, df_tuned], ignore_index=True)\n",
    "    ficha_detalle = os.path.join(OUT_DIR, \"ficha_detalle_120filas.csv\")\n",
    "    df_all.to_csv(ficha_detalle, index=False)\n",
    "    print(\"Guardado:\", ficha_detalle)\n",
    "\n",
    "    # Ficha resumen = 12 filas (6 modelos × 2 configs)\n",
    "    # (Aquí concatenamos summaries baseline y tuned)\n",
    "    resumen_12 = pd.concat([pd.read_csv(SUMMARY_BASE_CSV), pd.read_csv(SUMMARY_TUNED_CSV)], ignore_index=True)\n",
    "    ficha_resumen = os.path.join(OUT_DIR, \"ficha_resumen_12filas.csv\")\n",
    "    resumen_12.to_csv(ficha_resumen, index=False)\n",
    "    print(\"Guardado:\", ficha_resumen)\n",
    "\n",
    "    print(\"\\n=== FIN. Ya tienes todo para Fase 5 (tablas, deltas y curvas). ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
